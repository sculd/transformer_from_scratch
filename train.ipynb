{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "644783bb-4bd6-4421-ba93-2cee3e57fc3a",
   "metadata": {},
   "source": [
    "## prepare & process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "236114e6-5faa-41cb-8656-300188d10376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/alice.txt_cleaned.txt 143224 characters\n",
      "data/harry_potter_01.txt_cleaned.txt 454252 characters\n",
      "data/harry_potter_02.txt_cleaned.txt 509083 characters\n"
     ]
    }
   ],
   "source": [
    "import data\n",
    "import util\n",
    "\n",
    "filenames_list = [\"data/alice.txt\", \"data/harry_potter_01.txt\", \"data/harry_potter_02.txt\"]\n",
    "\n",
    "for filename in filenames_list:\n",
    "    data.clean_text(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ecf9b399-8290-4a80-84ab-ee9889001948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "characters: 26 tokens 6\n",
      "[18308, 14179, 373, 257, 18731, 13] -> Harry Potter was a wizard.\n",
      "18308\t -> Harry\n",
      "14179\t ->  Potter\n",
      "373\t ->  was\n",
      "257\t ->  a\n",
      "18731\t ->  wizard\n",
      "13\t -> .\n"
     ]
    }
   ],
   "source": [
    "text = \"Harry Potter was a wizard.\"\n",
    "\n",
    "tokens = util.tokenizer.encode(text)\n",
    "\n",
    "print(\"characters:\", len(text), \"tokens\", len(tokens))\n",
    "print(f\"{tokens} -> {util.tokenizer.decode(tokens)}\")\n",
    "for t in tokens:\n",
    "    print(f\"{t}\\t -> {util.tokenizer.decode([t])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce350c43-77f4-49fe-ac70-ca0a327885ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of tokens in txt: 35323\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/alice.txt_cleaned.txt\", 'r', encoding='utf-8-sig') as file: # remove BOM with -sig\n",
    "    txt = file.read()\n",
    "\n",
    "dataset = data.MyDataset(txt, max_length = 32, stride = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbebcdcd-6d38-4ebe-a437-ba0dff1d7986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "input: Project Gutenberg's Alice's Adventures in Wonderland, by Lewis Carroll This eBook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever. You\n",
      "target:  Gutenberg's Alice's Adventures in Wonderland, by Lewis Carroll This eBook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever. You may\n",
      "1\n",
      "input: 's Adventures in Wonderland, by Lewis Carroll This eBook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever. You may copy it,\n",
      "target:  Adventures in Wonderland, by Lewis Carroll This eBook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever. You may copy it, give\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print(f\"{i}\\ninput: {util.tokenizer.decode(dataset[i][0].tolist())}\\ntarget: {util.tokenizer.decode(dataset[i][1].tolist())}\")\n",
    "\n",
    "i = 1\n",
    "print(f\"{i}\\ninput: {util.tokenizer.decode(dataset[i][0].tolist())}\\ntarget: {util.tokenizer.decode(dataset[i][1].tolist())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "096af09e-aaaa-46a8-9ed1-58265acd5eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(dataset, batch_size=128, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5d7d5b1-676b-4c3a-8bdf-79d872c4c8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " word you fellows were saying.' 'Tell us a story!' said the March Hare. 'Yes, please do!' pleaded Alice. 'And be quick about it\n",
      " you fellows were saying.' 'Tell us a story!' said the March Hare. 'Yes, please do!' pleaded Alice. 'And be quick about it,'\n",
      " now,' she said, by way of keeping up the conversation a little. ''Tis so,' said the Duchess: 'and the moral of that is--\n",
      ",' she said, by way of keeping up the conversation a little. ''Tis so,' said the Duchess: 'and the moral of that is--\"\n"
     ]
    }
   ],
   "source": [
    "data_iter = iter(train_loader)\n",
    "\n",
    "x, y = next(data_iter)\n",
    "print(f\"{util.tokenizer.decode(x[0].tolist())}\\n{util.tokenizer.decode(y[0].tolist())}\")\n",
    "\n",
    "x, y = next(data_iter)\n",
    "print(f\"{util.tokenizer.decode(x[0].tolist())}\\n{util.tokenizer.decode(y[0].tolist())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b576140-bc30-4802-a9f5-81137ec42148",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e550944-f241-41e3-88a6-4516177cef54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import data\n",
    "import util\n",
    "import transformer\n",
    "import torch\n",
    "\n",
    "device = util.device\n",
    "print(device)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = transformer.GPTModel()\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "tokens_seen, global_step, n_epochs = 0, -1, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db97b58-f412-4be3-a60c-366ee83cbb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "train_loader = data.get_train_loader(\"data/alice.txt_cleaned.txt\")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    for input_batch, target_batch in train_loader:\n",
    "        optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "        input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "\n",
    "        logits = model(input_batch)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward() # Calculate loss gradients\n",
    "        optimizer.step() # Update model weights using loss gradients\n",
    "        tokens_seen += input_batch.numel()\n",
    "        global_step += 1\n",
    "\n",
    "        if global_step % 1000 == 0:\n",
    "            print(f\"Tokens seen: {tokens_seen}\")\n",
    "        # Optional evaluation step\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    losses.append(avg_loss)\n",
    "    print(f\"Epoch: {epoch + 1}, Loss: {avg_loss}\")\n",
    "    torch.save(model.state_dict(), \"model_\" + str(epoch + 1).zfill(3) + \".pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94ef998-1ce3-4466-91da-e69ec20c790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b35a9c-4daa-4ad1-b840-3294615148cb",
   "metadata": {},
   "source": [
    "## play with trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d8d00a0e-ac53-4265-86c4-747511f0a994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f29923f5-ba45-4cfd-bc0b-19ca3d75aed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "import transformer\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = transformer.GPTModel()\n",
    "model.to(util.device)\n",
    "model.load_state_dict(torch.load(\"model_snapshot/alice_txt_clean_txt_model_100.pth\", map_location=device, weights_only=True))\n",
    "model.eval() # q: this skips dropout?\n",
    "\n",
    "context_size = model.pos_emb.weight.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2023f851-80a1-485a-b187-db6700fd52dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Start context:  We're all\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : We're all mad here. I'm mad. You're mad.' 'How do you know I'm mad?' said Alice. 'You must be,' said the Cat, 'You must be,' said the Cat, 'or you might bite,' said the Cat\n",
      "1 : We're all mad here. I'm mad. You're mad.' 'How do you know I'm mad?' said Alice. 'You must be,' said the Cat, 'You must be,' said the Cat, 'or you might bite,' said the Cat\n",
      "2 : We're all mad here. I'm mad. You're mad.' 'How do you know I'm mad?' said Alice. 'You must be,' said the Cat, 'You must be,' said the Cat, 'or you can't talk about, '\n",
      "3 : We're all wrong.' 'Yes, but I grow at a reasonable pace,' said the Dormouse: 'not in that ridiculous fashion.' And he got up very grave such a minute or two, But she added in to the time he got up very grave\n",
      "4 : We're all wrong!' cried the Mock Turtle, capering wildly about. 'Change lobsters again!' yelled the Gryphon at the top of its voice. 'Back to itself, and the Mock Turtle. 'Back to its voice. 'Back to itself\n"
     ]
    }
   ],
   "source": [
    "import util\n",
    "\n",
    "start_context = input(\"Start context: \")\n",
    "\n",
    "idx = util.tokenizer.encode(start_context)\n",
    "idx = torch.tensor(idx).unsqueeze(0)\n",
    "device = util.device\n",
    "\n",
    "for i in range(5):\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=idx.to(device),\n",
    "        max_new_tokens=50,\n",
    "        context_size= context_size,\n",
    "        top_k=50,\n",
    "        temperature=0.5\n",
    "    )\n",
    "\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    out = util.tokenizer.decode(flat.tolist()).replace(\"\\n\", \" \")\n",
    "\n",
    "    print(i, \":\", out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b6d663-c882-4c47-8093-4fa83a3d441e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ce8d358d-d340-4faf-bb0d-1a17fe66ef04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "import transformer\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model_harry_potter_02 = transformer.GPTModel()\n",
    "model_harry_potter_02.to(util.device)\n",
    "model_harry_potter_02.load_state_dict(torch.load(\"model_snapshot/harry_potter_02_txt_cleaned_txt_model_step_15000.pt\", map_location=device, weights_only=True))\n",
    "model_harry_potter_02.eval() # q: this skips dropout?\n",
    "\n",
    "context_size = model_harry_potter_02.pos_emb.weight.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5272ef7c-1bf2-4791-ab18-7f4867968ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Start context:  gryffindor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : gryffindor, I suggest you look more closely at this.â€ Dumbledore reached across to Professor McGonagallâ€™s desk, looking around to say, but thought Harryâ€™s desk, when he kicked out of the bird looked\n",
      "1 : gryffindor Tower, desperate to tell Ron and Hermione about Colin and Dobby, but they werenâ€™t there. Harry left to look for them so far too many questions for them, wondering if only when heâ€ He pulled him out\n",
      "2 : gryffindor, I suggest you look more closely at this.â€ Dumbledore reached across to Professor McGonagallâ€™s desk, picked up the desk, picked up the hat, picked up the hat, the hat next to Hedwigâ\n",
      "3 : gryffindor, occasionally coming to long enough to copy down a name or date, then falling asleep again. He had been speaking for half an hour when something â€™s idea if it in case you know about six oâ€”â€\n",
      "4 : gryffindor Tower. The castle was quiet; it seemed that the feast was over. They walked past muttering portraits and creaking suits of armor, and climbed narrow flights and climbed narrow windows of armor, examining theICE Peeves broke into the pool of\n"
     ]
    }
   ],
   "source": [
    "import util\n",
    "\n",
    "start_context = input(\"Start context: \")\n",
    "\n",
    "idx = util.tokenizer.encode(start_context)\n",
    "idx = torch.tensor(idx).unsqueeze(0)\n",
    "device = util.device\n",
    "\n",
    "for i in range(5):\n",
    "    token_ids = generate(\n",
    "        model=model_harry_potter_02,\n",
    "        idx=idx.to(device),\n",
    "        max_new_tokens=50,\n",
    "        context_size= context_size,\n",
    "        top_k=50,\n",
    "        temperature=0.5\n",
    "    )\n",
    "\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    out = util.tokenizer.decode(flat.tolist()).replace(\"\\n\", \" \")\n",
    "\n",
    "    print(i, \":\", out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd2956a-caee-4e10-8252-bb40dbf5cac2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
