# transformer_from_scratch
This is implementation of transformer for self-study

## training
The training was done in the lambda cloud `gpu_1x_gh200`.
```
ubuntu@192-222-58-52:~/project/transformer_from_scratch$ python train.py
cuda
# of tokens in txt: 35323
Tokens seen: 4096
Epoch: 1, Loss: 5.804877758026123
Epoch: 2, Loss: 3.741780771928675
Epoch: 3, Loss: 2.346310349071727
Epoch: 4, Loss: 1.0587690420010512
Epoch: 5, Loss: 0.4345137910807834
Epoch: 6, Loss: 0.2750820754205479
Epoch: 7, Loss: 0.2264650225201074
Epoch: 8, Loss: 0.20463062559857087
Epoch: 9, Loss: 0.1895058748914915
Epoch: 10, Loss: 0.18003187350490513
Epoch: 11, Loss: 0.17463716404402957
Epoch: 12, Loss: 0.1692803055047989
Epoch: 13, Loss: 0.16503132068935564
...
Epoch: 63, Loss: 0.13082189792219331
Epoch: 64, Loss: 0.13135370513533845
Epoch: 65, Loss: 0.1297982819378376
...
Epoch: 99, Loss: 0.12265084300409346
Epoch: 100, Loss: 0.12297938555917319
```


